{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"vscode\"\n",
    "import plotly.graph_objects as go\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from equivariant_pose_graph.dataset.point_cloud_data_module import MultiviewDataModule\n",
    "from equivariant_pose_graph.utils.se3 import random_se3\n",
    "from equivariant_pose_graph.utils.load_model_utils import load_merged_model\n",
    "from equivariant_pose_graph.utils.visualizations import plot_all_predictions\n",
    "from equivariant_pose_graph.training.flow_equivariance_training_module_nocentering_multimodal import EquivarianceTrainingModule, EquivarianceTrainingModule_WithPZCondX\n",
    "from equivariant_pose_graph.dataset.point_cloud_data_module import MultiviewDataModule\n",
    "from equivariant_pose_graph.models.transformer_flow import ResidualFlow_DiffEmbTransformer\n",
    "from equivariant_pose_graph.models.multimodal_transformer_flow import Multimodal_ResidualFlow_DiffEmbTransformer, Multimodal_ResidualFlow_DiffEmbTransformer_WithPZCondX\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "def toDisplay(x, target_dim = 2):\n",
    "    while(x.dim() > target_dim):\n",
    "        x = x[0]\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def plot_multi_np(plist):\n",
    "    \"\"\"\n",
    "    Args: plist, list of numpy arrays of shape, (1,num_points,3)\n",
    "    \"\"\"\n",
    "    colors = [\n",
    "        '#1f77b4',  # muted blue\n",
    "        '#ff7f0e',  # safety orange\n",
    "        '#2ca02c',  # cooked asparagus green\n",
    "        '#d62728',  # brick red\n",
    "        '#9467bd',  # muted purple\n",
    "        '#e377c2',  # raspberry yogurt pink\n",
    "        '#8c564b',  # chestnut brown\n",
    "        '#7f7f7f',  # middle gray\n",
    "        '#bcbd22',  # curry yellow-green\n",
    "        '#17becf'   # blue-teal\n",
    "    ]\n",
    "    skip = 1\n",
    "    go_data = []\n",
    "    for i in range(len(plist)):\n",
    "        p_dp = toDisplay(torch.from_numpy(plist[i]))\n",
    "        plot = go.Scatter3d(x=p_dp[::skip,0], y=p_dp[::skip,1], z=p_dp[::skip,2], \n",
    "                     mode='markers', marker=dict(size=2, color=colors[i],\n",
    "                     symbol='circle'))\n",
    "        go_data.append(plot)\n",
    " \n",
    "    layout = go.Layout(\n",
    "        scene=dict(\n",
    "            aspectmode='data'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=go_data, layout=layout)\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "def get_dm(cfg):\n",
    "    dm = MultiviewDataModule(\n",
    "        dataset_root=cfg.dataset_root,\n",
    "        test_dataset_root=cfg.test_dataset_root,\n",
    "        dataset_index=cfg.dataset_index,\n",
    "        action_class=cfg.action_class,\n",
    "        anchor_class=cfg.anchor_class,\n",
    "        dataset_size=cfg.dataset_size,\n",
    "        rotation_variance=np.pi/180 * cfg.rotation_variance,\n",
    "        translation_variance=cfg.translation_variance,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.num_workers,\n",
    "        cloud_type=cfg.cloud_type,\n",
    "        num_points=cfg.num_points,\n",
    "        overfit=cfg.overfit,\n",
    "        overfit_distractor_aug=cfg.overfit_distractor_aug,\n",
    "        num_overfit_transforms=cfg.num_overfit_transforms,\n",
    "        seed_overfit_transforms=cfg.seed_overfit_transforms,\n",
    "        set_Y_transform_to_identity=cfg.set_Y_transform_to_identity,\n",
    "        set_Y_transform_to_overfit=cfg.set_Y_transform_to_overfit,\n",
    "        num_demo=cfg.num_demo,\n",
    "        synthetic_occlusion=cfg.synthetic_occlusion,\n",
    "        ball_radius=cfg.ball_radius,\n",
    "        plane_standoff=cfg.plane_standoff,\n",
    "        bottom_surface_z_clipping_height=cfg.bottom_surface_z_clipping_height,\n",
    "        scale_point_clouds=cfg.scale_point_clouds,\n",
    "        scale_point_clouds_min=cfg.scale_point_clouds_min,\n",
    "        scale_point_clouds_max=cfg.scale_point_clouds_max,\n",
    "        distractor_anchor_aug=cfg.distractor_anchor_aug,\n",
    "        demo_mod_k_range=[cfg.demo_mod_k_range_min, cfg.demo_mod_k_range_max],\n",
    "        demo_mod_rot_var=cfg.demo_mod_rot_var * np.pi/180,\n",
    "        demo_mod_trans_var=cfg.demo_mod_trans_var,\n",
    "        multimodal_transform_base=cfg.multimodal_transform_base,\n",
    "        action_rot_sample_method=cfg.action_rot_sample_method,\n",
    "        anchor_rot_sample_method=cfg.anchor_rot_sample_method,\n",
    "        distractor_rot_sample_method=cfg.distractor_rot_sample_method,\n",
    "        skip_failed_occlusion=cfg.skip_failed_occlusion,\n",
    "        min_num_cameras=cfg.min_num_cameras,\n",
    "        max_num_cameras=cfg.max_num_cameras,\n",
    "        use_consistent_validation_set=cfg.use_consistent_validation_set,\n",
    "        use_all_validation_sets=cfg.use_all_validation_sets,\n",
    "        conval_rotation_variance=np.pi/180 * cfg.conval_rotation_variance,\n",
    "        conval_translation_variance=cfg.conval_translation_variance,\n",
    "        conval_synthetic_occlusion=cfg.conval_synthetic_occlusion,\n",
    "        conval_scale_point_clouds=cfg.conval_scale_point_clouds,\n",
    "        conval_action_rot_sample_method=cfg.conval_action_rot_sample_method,\n",
    "        conval_anchor_rot_sample_method=cfg.conval_anchor_rot_sample_method,\n",
    "        conval_distractor_rot_sample_method=cfg.conval_distractor_rot_sample_method,\n",
    "        conval_min_num_cameras=cfg.conval_min_num_cameras,\n",
    "        conval_max_num_cameras=cfg.conval_max_num_cameras,\n",
    "        conval_downsample_type=cfg.conval_downsample_type,\n",
    "        conval_gaussian_noise_mu=cfg.conval_gaussian_noise_mu,\n",
    "        conval_gaussian_noise_std=cfg.conval_gaussian_noise_std,\n",
    "        use_class_labels=cfg.use_class_labels,\n",
    "        action_occlusion_class=cfg.action_occlusion_class,\n",
    "        action_plane_occlusion=cfg.action_plane_occlusion,\n",
    "        action_ball_occlusion=cfg.action_ball_occlusion,\n",
    "        action_bottom_surface_occlusion=cfg.action_bottom_surface_occlusion,\n",
    "        anchor_occlusion_class=cfg.anchor_occlusion_class,\n",
    "        anchor_plane_occlusion=cfg.anchor_plane_occlusion,\n",
    "        anchor_ball_occlusion=cfg.anchor_ball_occlusion,\n",
    "        anchor_bottom_surface_occlusion=cfg.anchor_bottom_surface_occlusion,\n",
    "        downsample_type=cfg.downsample_type,\n",
    "        gaussian_noise_mu=cfg.gaussian_noise_mu,\n",
    "        gaussian_noise_std=cfg.gaussian_noise_std,\n",
    "    )\n",
    "\n",
    "    dm.setup()\n",
    "    return dm\n",
    "\n",
    "def get_model(cfg):\n",
    "\n",
    "    TP_input_dims = Multimodal_ResidualFlow_DiffEmbTransformer.TP_INPUT_DIMS[cfg.conditioning]\n",
    "    if cfg.conditioning in [\"latent_z_linear\", \"hybrid_pos_delta_l2norm\"]:\n",
    "        TP_input_dims += cfg.latent_z_linear_size # Hacky way to add the dynamic latent z to the input dims\n",
    "\n",
    "    # if cfg.conditioning in [\"latent_z_linear\"]:\n",
    "    #     assert not cfg.freeze_embnn and not cfg.freeze_z_embnn and not cfg.freeze_residual_flow, \"Probably don't want to freeze the network when training the latent model\"\n",
    "    inner_network = ResidualFlow_DiffEmbTransformer(\n",
    "        emb_dims=cfg.emb_dims,\n",
    "        input_dims=TP_input_dims,\n",
    "        emb_nn=cfg.emb_nn,\n",
    "        return_flow_component=cfg.return_flow_component,\n",
    "        center_feature=cfg.center_feature,\n",
    "        inital_sampling_ratio=cfg.inital_sampling_ratio,\n",
    "        pred_weight=cfg.pred_weight,\n",
    "        freeze_embnn=cfg.freeze_embnn,\n",
    "        conditioning_size=cfg.latent_z_linear_size if cfg.conditioning in [\"latent_z_linear_internalcond\", \"hybrid_pos_delta_l2norm_internalcond\", \"hybrid_pos_delta_l2norm_global_internalcond\"] else 0,\n",
    "        multilaterate=cfg.multilaterate,\n",
    "        sample=cfg.mlat_sample,\n",
    "        mlat_nkps=cfg.mlat_nkps,\n",
    "        pred_mlat_weight=cfg.pred_mlat_weight,\n",
    "        conditioning_type=cfg.taxpose_conditioning_type,\n",
    "        flow_head_use_weighted_sum=cfg.flow_head_use_weighted_sum,\n",
    "        flow_head_use_selected_point_feature=cfg.flow_head_use_selected_point_feature,\n",
    "        post_encoder_input_dims=cfg.post_encoder_input_dims,\n",
    "        flow_direction=cfg.flow_direction,\n",
    "        )\n",
    "\n",
    "    network = Multimodal_ResidualFlow_DiffEmbTransformer(\n",
    "        residualflow_diffembtransformer=inner_network,\n",
    "        gumbel_temp=cfg.gumbel_temp,\n",
    "        freeze_residual_flow=cfg.freeze_residual_flow,\n",
    "        center_feature=cfg.center_feature,\n",
    "        freeze_z_embnn=cfg.freeze_z_embnn,\n",
    "        division_smooth_factor=cfg.division_smooth_factor,\n",
    "        add_smooth_factor=cfg.add_smooth_factor,\n",
    "        conditioning=cfg.conditioning,\n",
    "        latent_z_linear_size=cfg.latent_z_linear_size,\n",
    "        taxpose_centering=cfg.taxpose_centering,\n",
    "        use_action_z=cfg.use_action_z,\n",
    "        pzY_encoder_type=cfg.pzY_encoder_type,\n",
    "        pzY_dropout_goal_emb=cfg.pzY_dropout_goal_emb,\n",
    "        pzY_transformer=cfg.pzY_transformer,\n",
    "        pzY_transformer_embnn_dims=cfg.pzY_transformer_embnn_dims,\n",
    "        pzY_transformer_emb_dims=cfg.pzY_transformer_emb_dims,\n",
    "        pzY_input_dims=cfg.pzY_input_dims,\n",
    "        hybrid_cond_logvar_limit=cfg.hybrid_cond_logvar_limit,\n",
    "    )\n",
    "\n",
    "    model = EquivarianceTrainingModule(\n",
    "        network,\n",
    "        lr=cfg.lr,\n",
    "        image_log_period=cfg.image_logging_period,\n",
    "        flow_supervision=cfg.flow_supervision,\n",
    "        point_loss_type=cfg.point_loss_type,\n",
    "        action_weight=cfg.action_weight,\n",
    "        anchor_weight=cfg.anchor_weight,\n",
    "        displace_weight=cfg.displace_weight,\n",
    "        consistency_weight=cfg.consistency_weight,\n",
    "        smoothness_weight=cfg.smoothness_weight,\n",
    "        rotation_weight=cfg.rotation_weight,\n",
    "        #latent_weight=cfg.latent_weight,\n",
    "        weight_normalize=cfg.weight_normalize,\n",
    "        softmax_temperature=cfg.softmax_temperature,\n",
    "        vae_reg_loss_weight=cfg.vae_reg_loss_weight,\n",
    "        sigmoid_on=cfg.sigmoid_on,\n",
    "        min_err_across_racks_debug=cfg.min_err_across_racks_debug,\n",
    "        error_mode_2rack=cfg.error_mode_2rack,\n",
    "        n_samples=cfg.pzY_n_samples,\n",
    "        get_errors_across_samples=cfg.pzY_get_errors_across_samples,\n",
    "        use_debug_sampling_methods=cfg.pzY_use_debug_sampling_methods,\n",
    "        return_flow_component=cfg.return_flow_component,\n",
    "        plot_encoder_distribution=cfg.plot_encoder_distribution,\n",
    "        joint_infonce_loss_weight=cfg.pzY_joint_infonce_loss_weight,\n",
    "        spatial_distance_regularization_type=cfg.spatial_distance_regularization_type,\n",
    "        spatial_distance_regularization_weight=cfg.spatial_distance_regularization_weight,\n",
    "        hybrid_cond_regularize_all=cfg.hybrid_cond_regularize_all,\n",
    "        pzY_taxpose_infonce_loss_weight=cfg.pzY_taxpose_infonce_loss_weight,\n",
    "        pzY_taxpose_occ_infonce_loss_weight=cfg.pzY_taxpose_occ_infonce_loss_weight,\n",
    "    )\n",
    "\n",
    "    if not cfg.pzX_adversarial and not cfg.joint_train_prior and cfg.init_cond_x and (not cfg.freeze_embnn or not cfg.freeze_residual_flow):\n",
    "        raise ValueError(\"YOU PROBABLY DIDN'T MEAN TO DO JOINT TRAINING\")\n",
    "    if not cfg.joint_train_prior and cfg.init_cond_x and cfg.checkpoint_file is None:\n",
    "        raise ValueError(\"YOU PROBABLY DIDN'T MEAN TO TRAIN BOTH P(Z|X) AND P(Z|Y) FROM SCRATCH\")\n",
    "    \n",
    "    if cfg.init_cond_x:\n",
    "        network_cond_x = Multimodal_ResidualFlow_DiffEmbTransformer_WithPZCondX(\n",
    "            residualflow_embnn=network,\n",
    "            encoder_type=cfg.pzcondx_encoder_type,\n",
    "            shuffle_for_pzX=cfg.shuffle_for_pzX,\n",
    "            use_action_z=cfg.use_action_z,\n",
    "            pzX_transformer=cfg.pzX_transformer,\n",
    "            pzX_transformer_embnn_dims=cfg.pzX_transformer_embnn_dims,\n",
    "            pzX_transformer_emb_dims=cfg.pzX_transformer_emb_dims,\n",
    "            pzX_input_dims=cfg.pzX_input_dims,\n",
    "            pzX_dropout_goal_emb=cfg.pzX_dropout_goal_emb,\n",
    "            hybrid_cond_pzX_sample_latent=cfg.hybrid_cond_pzX_sample_latent,\n",
    "        )\n",
    "\n",
    "        model_cond_x = EquivarianceTrainingModule_WithPZCondX(\n",
    "            network_cond_x,\n",
    "            model,\n",
    "            goal_emb_cond_x_loss_weight=cfg.goal_emb_cond_x_loss_weight,\n",
    "            joint_train_prior=cfg.joint_train_prior,\n",
    "            freeze_residual_flow=cfg.freeze_residual_flow,\n",
    "            freeze_z_embnn=cfg.freeze_z_embnn,\n",
    "            freeze_embnn=cfg.freeze_embnn,\n",
    "            n_samples=cfg.pzX_n_samples,\n",
    "            get_errors_across_samples=cfg.pzX_get_errors_across_samples,\n",
    "            use_debug_sampling_methods=cfg.pzX_use_debug_sampling_methods,\n",
    "            plot_encoder_distribution=cfg.plot_encoder_distribution,\n",
    "            pzX_use_pzY_z_samples=cfg.pzX_use_pzY_z_samples,\n",
    "            goal_emb_cond_x_loss_type=cfg.goal_emb_cond_x_loss_type,\n",
    "            joint_infonce_loss_weight=cfg.pzX_joint_infonce_loss_weight,\n",
    "            spatial_distance_regularization_type=cfg.spatial_distance_regularization_type,\n",
    "            spatial_distance_regularization_weight=cfg.spatial_distance_regularization_weight,\n",
    "            overwrite_loss=cfg.pzX_overwrite_loss,\n",
    "            pzX_adversarial=cfg.pzX_adversarial,\n",
    "            hybrid_cond_pzX_regularize_type=cfg.hybrid_cond_pzX_regularize_type,\n",
    "            hybrid_cond_pzX_sample_latent=cfg.hybrid_cond_pzX_sample_latent,\n",
    "        )\n",
    "\n",
    "        model_cond_x.cuda()\n",
    "        model_cond_x.eval()        \n",
    "    else:\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "\n",
    "    if(cfg.checkpoint_file is not None):\n",
    "        print(\"loaded checkpoint from\")\n",
    "        print(cfg.checkpoint_file)\n",
    "        if not cfg.load_cond_x:\n",
    "            model.load_state_dict(torch.load(cfg.checkpoint_file)['state_dict'])\n",
    "            \n",
    "            if cfg.init_cond_x and cfg. load_pretraining_for_conditioning:\n",
    "                if cfg.checkpoint_file_action is not None:\n",
    "                    if model_cond_x.model_with_cond_x.encoder_type == \"1_dgcnn\":\n",
    "                        raise NotImplementedError()\n",
    "                    elif model_cond_x.model_with_cond_x.encoder_type == \"2_dgcnn\":\n",
    "                        model_cond_x.model_with_cond_x.p_z_cond_x_embnn_action.conv5 = nn.Conv2d(512, 512, kernel_size=1, bias=False)\n",
    "                        model_cond_x.model_with_cond_x.p_z_cond_x_embnn_action.bn5 = nn.BatchNorm2d(512)\n",
    "                        model_cond_x.model_with_cond_x.p_z_cond_x_embnn_action.load_state_dict(\n",
    "                            torch.load(cfg.checkpoint_file_action)['embnn_state_dict'])\n",
    "                        model_cond_x.model_with_cond_x.p_z_cond_x_embnn_action.conv5 = nn.Conv2d(512, TP_input_dims-3, kernel_size=1, bias=False)\n",
    "                        model_cond_x.model_with_cond_x.p_z_cond_x_embnn_action.bn5 = nn.BatchNorm2d(TP_input_dims-3)\n",
    "                        print(\"----Action Pretraining for p(z|X) Loaded!----\")\n",
    "                    else:\n",
    "                        raise ValueError()\n",
    "                if cfg.checkpoint_file_anchor is not None:\n",
    "                    if model_cond_x.model_with_cond_x.encoder_type == \"1_dgcnn\":\n",
    "                        raise NotImplementedError()\n",
    "                    elif model_cond_x.model_with_cond_x.encoder_type == \"2_dgcnn\":\n",
    "                        print(\"--Not loading p(z|X) for anchor for now--\")\n",
    "                        pass\n",
    "                        # model_cond_x.model_with_cond_x.p_z_cond_x_embnn_anchor.conv5 = nn.Conv2d(512, 512, kernel_size=1, bias=False)\n",
    "                        # model_cond_x.model_with_cond_x.p_z_cond_x_embnn_anchor.bn5 = nn.BatchNorm2d(512)\n",
    "                        # model_cond_x.model_with_cond_x.p_z_cond_x_embnn_anchor.load_state_dict(\n",
    "                        #     torch.load(cfg.checkpoint_file_anchor)['embnn_state_dict'])\n",
    "                        # model_cond_x.model_with_cond_x.p_z_cond_x_embnn_anchor.conv5 = nn.Conv2d(512, TP_input_dims-3, kernel_size=1, bias=False)\n",
    "                        # model_cond_x.model_with_cond_x.p_z_cond_x_embnn_anchor.bn5 = nn.BatchNorm2d(TP_input_dims-3)\n",
    "                        # print(\"--Anchor Pretraining for p(z|X) Loaded!--\")\n",
    "                    else:\n",
    "                        raise ValueError()\n",
    "        else:\n",
    "            model_cond_x.load_state_dict(torch.load(cfg.checkpoint_file)['state_dict'])\n",
    "\n",
    "    else:\n",
    "        if cfg.checkpoint_file_action is not None:\n",
    "            if cfg.load_pretraining_for_taxpose:\n",
    "                model.model.tax_pose.emb_nn_action.conv1 = nn.Conv2d(3*2, 64, kernel_size=1, bias=False)\n",
    "                model.model.tax_pose.emb_nn_action.load_state_dict(\n",
    "                    torch.load(cfg.checkpoint_file_action)['embnn_state_dict'])\n",
    "                model.model.tax_pose.emb_nn_action.conv1 = nn.Conv2d(TP_input_dims*2, 64, kernel_size=1, bias=False)\n",
    "                print(\n",
    "                '-----------------------Pretrained EmbNN Action Model Loaded!-----------------------')\n",
    "            if cfg.load_pretraining_for_conditioning:\n",
    "                if not cfg.init_cond_x:\n",
    "                    print(\"---Not Loading p(z|Y) Pretraining For Now---\")\n",
    "                    pass\n",
    "                    # model.model.emb_nn_objs_at_goal.conv5 = nn.Conv2d(512, 512, kernel_size=1, bias=False)\n",
    "                    # model.model.emb_nn_objs_at_goal.bn5 = nn.BatchNorm2d(512)\n",
    "                    # model.model.emb_nn_objs_at_goal.load_state_dict(\n",
    "                    #         torch.load(cfg.checkpoint_file_action)['embnn_state_dict'])\n",
    "                    # model.model.emb_nn_objs_at_goal.conv5 = nn.Conv2d(512, TP_input_dims-3, kernel_size=1, bias=False)\n",
    "                    # model.model.emb_nn_objs_at_goal.bn5 = nn.BatchNorm2d(TP_input_dims-3)\n",
    "                    # print(\"----Action Pretraining for p(z|Y) Loaded!----\")\n",
    "            \n",
    "        if cfg.checkpoint_file_anchor is not None:\n",
    "            if cfg.load_pretraining_for_taxpose:\n",
    "                model.model.tax_pose.emb_nn_anchor.conv1 = nn.Conv2d(3*2, 64, kernel_size=1, bias=False)\n",
    "                model.model.tax_pose.emb_nn_anchor.load_state_dict(\n",
    "                    torch.load(cfg.checkpoint_file_anchor)['embnn_state_dict'])\n",
    "                model.model.tax_pose.emb_nn_anchor.conv1 = nn.Conv2d(TP_input_dims*2, 64, kernel_size=1, bias=False)\n",
    "                print(\n",
    "                '-----------------------Pretrained EmbNN Anchor Model Loaded!-----------------------')\n",
    "            if cfg.load_pretraining_for_conditioning:\n",
    "                if not cfg.init_cond_x:\n",
    "                    print(\"---Not Loading p(z|Y) Pretraining For Now---\")\n",
    "                    pass\n",
    "                    # if cfg.checkpoint_file_action is None:\n",
    "                    #     model.model.emb_nn_objs_at_goal.conv5 = nn.Conv2d(512, 512, kernel_size=1, bias=False)\n",
    "                    #     model.model.emb_nn_objs_at_goal.bn5 = nn.BatchNorm2d(512)\n",
    "                    #     model.model.emb_nn_objs_at_goal.load_state_dict(\n",
    "                    #             torch.load(cfg.checkpoint_file_action)['embnn_state_dict'])\n",
    "                    #     model.model.emb_nn_objs_at_goal.conv5 = nn.Conv2d(512, TP_input_dims-3, kernel_size=1, bias=False)\n",
    "                    #     model.model.emb_nn_objs_at_goal.bn5 = nn.BatchNorm2d(TP_input_dims-3)\n",
    "                    #     print(\"----Anchor Pretraining for p(z|Y) Loaded! (because action pretraining is not present)----\")\n",
    "\n",
    "    cfg.return_debug = True\n",
    "    # model = load_merged_model(pzY_checkpoint_path, pzX_checkpoint_path, cfg.conditioning, True, cfg)\n",
    "    # model.cuda()\n",
    "\n",
    "    if cfg.init_cond_x:\n",
    "        model = model_cond_x\n",
    "    return model\n",
    "\n",
    "###########################################################################################################\n",
    "\n",
    "# cfg = OmegaConf.load(\"/home/odonca/workspace/rpad/equivariant_pose_graph/configs/experimental/train_pzX-dgcnn-transformer-gradclip1-se3-action-upright-anchor_pzY-pn2_gradclip1e-3_uniformz-action_uniformz-anchor_noreg.yaml\")\n",
    "# cfg.checkpoint_file = '/home/odonca/workspace/rpad/data/equivariant_pose_graph/logs/residual_flow_occlusion/2024-01-22_223338/residual_flow_occlusion/rsacw3qn/checkpoints/epoch_2000_global_step_250000.ckpt'\n",
    "cfg = OmegaConf.load(\"/home/odonca/workspace/rpad/equivariant_pose_graph/configs/evalgap/joint_train_pzX-dgcnn-transformer_pzY-pn2_gc1e-3_se3-upright_noreg_flow-fix-one-head_with-cl_synthocc0.8_2rackvariety.yaml\")\n",
    "cfg.checkpoint_file = '/home/odonca/workspace/rpad/data/equivariant_pose_graph/logs/residual_flow_occlusion/2024-02-14_001809/residual_flow_occlusion/n9a6o3sz/checkpoints/epoch_838_global_step_140000.ckpt'\n",
    "cfg.load_cond_x = True\n",
    "cfg.batch_size = 1\n",
    "\n",
    "dm = get_dm(cfg)\n",
    "\n",
    "val_dataloader = dm.val_dataloader()[0]\n",
    "val_iter = iter(val_dataloader)\n",
    "\n",
    "model = get_model(cfg)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import Transform3d, Translate, matrix_to_axis_angle, Rotate, random_rotations\n",
    "from equivariant_pose_graph.utils.se3 import dualflow2pose, flow2pose, get_translation, get_degree_angle, dense_flow_loss, pure_translation_se3\n",
    "\n",
    "print(val_iter)\n",
    "\n",
    "# Get the data\n",
    "batch = next(val_iter)\n",
    "points_action = batch['points_action'].to(model.device)\n",
    "points_anchor = batch['points_anchor'].to(model.device)\n",
    "points_trans_action = batch['points_action_trans'].to(model.device)\n",
    "points_trans_anchor = batch['points_anchor_trans'].to(model.device)\n",
    "points_onetrans_action = batch['points_action_onetrans'].to(model.device) if 'points_action_onetrans' in batch else batch['points_action'].to(model.device)\n",
    "points_onetrans_anchor = batch['points_anchor_onetrans'].to(model.device) if 'points_anchor_onetrans' in batch else batch['points_anchor'].to(model.device)\n",
    "\n",
    "T0 = Transform3d(matrix=batch['T0']).to(model.device)\n",
    "T1 = Transform3d(matrix=batch['T1']).to(model.device)\n",
    "\n",
    "print(f'points_trans_action.shape: {points_trans_action.shape}')\n",
    "print(f'points_trans_anchor.shape: {points_trans_anchor.shape}')\n",
    "\n",
    "# Run the model\n",
    "model_outputs = model.model_with_cond_x(points_trans_action, \n",
    "                            points_trans_anchor, \n",
    "                            None, \n",
    "                            None, \n",
    "                            n_samples=1, \n",
    "                            sampling_method='gumbel')\n",
    "\n",
    "all_predicted_points = [points_action[0][:, :3].detach().cpu().numpy(), \n",
    "                        T1.inverse().transform_points(points_trans_anchor[:, :, :3])[0].detach().cpu().numpy()]\n",
    "for model_output in model_outputs:\n",
    "    x_action = model_output['flow_action']\n",
    "    x_anchor = model_output['flow_anchor']\n",
    "    goal_emb = model_output['goal_emb']\n",
    "\n",
    "    # Get the prediction from the model forward pass\n",
    "    points_action = points_action[:, :, :3]\n",
    "    points_anchor = points_anchor[:, :, :3]\n",
    "    points_trans_action = points_trans_action[:, :, :3]\n",
    "    points_trans_anchor = points_trans_anchor[:, :, :3]\n",
    "\n",
    "    if \"sampled_ixs_action\" in model_outputs[0]:\n",
    "        ixs_action = model_outputs[0][\"sampled_ixs_action\"].unsqueeze(-1)\n",
    "        sampled_points_action = torch.take_along_dim(\n",
    "            points_action, ixs_action, dim=1\n",
    "        )\n",
    "        sampled_points_trans_action = torch.take_along_dim(\n",
    "            points_trans_action, ixs_action, dim=1\n",
    "        )\n",
    "    else:\n",
    "        sampled_points_action = points_action\n",
    "        sampled_points_trans_action = points_trans_action\n",
    "\n",
    "    if \"sampled_ixs_anchor\" in model_outputs[0]:\n",
    "        ixs_anchor = model_outputs[0][\"sampled_ixs_anchor\"].unsqueeze(-1)\n",
    "        sampled_points_anchor = torch.take_along_dim(\n",
    "            points_anchor, ixs_anchor, dim=1\n",
    "        )\n",
    "        sampled_points_trans_anchor = torch.take_along_dim(\n",
    "            points_trans_anchor, ixs_anchor, dim=1\n",
    "        )\n",
    "    else:\n",
    "        sampled_points_anchor = points_anchor\n",
    "        sampled_points_trans_anchor = points_trans_anchor\n",
    "\n",
    "    pred_flow_action, pred_w_action = model.extract_flow_and_weight(x_action)\n",
    "    pred_flow_anchor, pred_w_anchor = model.extract_flow_and_weight(x_anchor)\n",
    "\n",
    "    pred_T_action = dualflow2pose(xyz_src=sampled_points_trans_action, \n",
    "                                xyz_tgt=sampled_points_trans_anchor,\n",
    "                                flow_src=pred_flow_action, \n",
    "                                flow_tgt=pred_flow_anchor,\n",
    "                                weights_src=pred_w_action, \n",
    "                                weights_tgt=pred_w_anchor,\n",
    "                                return_transform3d=True, \n",
    "                                normalization_scehme=\"softmax\",\n",
    "                                temperature=1)\n",
    "\n",
    "    pred_points_action = pred_T_action.transform_points(points_trans_action)\n",
    "\n",
    "    all_predicted_points.append(T1.inverse().transform_points(pred_points_action)[0].detach().cpu().numpy())\n",
    "    # all_predicted_points.append(pred_points_action[0].detach().cpu().numpy())\n",
    "\n",
    "plot_multi_np(all_predicted_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# 2 rack arbitrary\n",
    "# eval_dir = '/home/odonca/workspace/rpad/taxpose_j/logs/eval_mug_arbitrary_10/2024-02-07/11-49-59/pointclouds'\n",
    "# 2 rack arbitrary\n",
    "eval_dir = '/home/odonca/workspace/rpad/taxpose_j/logs/eval_mug_arbitrary_10/2024-02-14/13-20-16/pointclouds'\n",
    "\n",
    "def load_eval_data(path):\n",
    "    filenames = glob.glob(os.path.join(path, '*_init_obj_points.npz'))\n",
    "    \n",
    "    action_points_list = []\n",
    "    anchor_points_list = []\n",
    "    \n",
    "    for filename in filenames:\n",
    "        data = np.load(filename)\n",
    "        # action_points.append(data['points_mug_raw'])\n",
    "        # anchor_points.append(data['points_rack_raw'])\n",
    "        \n",
    "        point_data = np.load(filename, allow_pickle=True)\n",
    "        points_raw_np = point_data['clouds']\n",
    "        classes_raw_np = point_data['classes']\n",
    "        points_action_np = points_raw_np[classes_raw_np == 0].copy()\n",
    "        points_action_mean_np = points_action_np.mean(axis=0)\n",
    "        points_action_np = points_action_np - points_action_mean_np\n",
    "\n",
    "        points_anchor_np = points_raw_np[classes_raw_np == 1].copy()\n",
    "        points_anchor_np = points_anchor_np - points_action_mean_np\n",
    "        points_anchor_mean_np = points_anchor_np.mean(axis=0)\n",
    "\n",
    "        points_action = torch.from_numpy(points_action_np).float().unsqueeze(0)\n",
    "        points_anchor = torch.from_numpy(points_anchor_np).float().unsqueeze(0)\n",
    "        \n",
    "        action_points_list.append(points_action)\n",
    "        anchor_points_list.append(points_anchor)\n",
    "    \n",
    "    return action_points_list, anchor_points_list\n",
    "    \n",
    "action_points_list, anchor_points_list = load_eval_data(eval_dir)\n",
    "print(f'len(action_points_list): {len(action_points_list)}')\n",
    "\n",
    "action_points_iter = iter(action_points_list)\n",
    "anchor_points_iter = iter(anchor_points_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equivariant_pose_graph.utils.occlusion_utils import ball_occlusion, plane_occlusion, bottom_surface_occlusion\n",
    "from pytorch3d.ops import sample_farthest_points\n",
    "from equivariant_pose_graph.utils.visualizations import plot_taxposed_embeddings\n",
    "\n",
    "action_points = next(action_points_iter)\n",
    "anchor_points = next(anchor_points_iter)\n",
    "\n",
    "print(f'action_points.shape: {action_points.shape}')\n",
    "print(f'anchor_points.shape: {anchor_points.shape}')\n",
    "\n",
    "if False:\n",
    "    # action_points = ball_occlusion(action_points[0], radius=cfg.ball_radius).unsqueeze(0)\n",
    "    # anchor_points = ball_occlusion(anchor_points[0], radius=cfg.ball_radius).unsqueeze(0)\n",
    "    \n",
    "    action_points = plane_occlusion(action_points[0], stand_off=cfg.plane_standoff).unsqueeze(0)\n",
    "    # anchor_points = plane_occlusion(anchor_points[0], stand_off=cfg.plane_standoff).unsqueeze(0)\n",
    "    \n",
    "action_points, _ = sample_farthest_points(action_points, K=cfg.num_points, random_start_point=True)\n",
    "anchor_points, _ = sample_farthest_points(anchor_points, K=cfg.num_points, random_start_point=True)\n",
    "\n",
    "action_points = action_points.numpy()\n",
    "anchor_points = anchor_points.numpy()\n",
    "\n",
    "print(f'action_points.shape: {action_points.shape}')\n",
    "print(f'anchor_points.shape: {anchor_points.shape}')\n",
    "\n",
    "action_points = torch.from_numpy(action_points).to(model.device)\n",
    "anchor_points = torch.from_numpy(anchor_points).to(model.device)\n",
    "\n",
    "print(action_points.shape)\n",
    "print(anchor_points.shape)\n",
    "\n",
    "# Run the model\n",
    "model_outputs = model.model_with_cond_x(action_points, \n",
    "                            anchor_points, \n",
    "                            None, \n",
    "                            None, \n",
    "                            n_samples=1, \n",
    "                            sampling_method='gumbel')\n",
    "\n",
    "all_predicted_points = [action_points[0].detach().cpu().numpy(), \n",
    "                        anchor_points[0].detach().cpu().numpy()]\n",
    "for model_output in model_outputs:\n",
    "    x_action = model_output['flow_action']\n",
    "    x_anchor = model_output['flow_anchor']\n",
    "    goal_emb = model_output['goal_emb']\n",
    "    goal_emb_cond_x = model_output['goal_emb_cond_x']\n",
    "\n",
    "    # Get the prediction from the model forward pass\n",
    "    points_action = points_action[:, :, :3]\n",
    "    points_anchor = points_anchor[:, :, :3]\n",
    "    action_points = action_points[:, :, :3]\n",
    "    anchor_points = anchor_points[:, :, :3]\n",
    "\n",
    "    if \"sampled_ixs_action\" in model_outputs[0]:\n",
    "        ixs_action = model_outputs[0][\"sampled_ixs_action\"].unsqueeze(-1)\n",
    "        sampled_points_action = torch.take_along_dim(\n",
    "            points_action, ixs_action, dim=1\n",
    "        )\n",
    "        sampled_action_points = torch.take_along_dim(\n",
    "            action_points, ixs_action, dim=1\n",
    "        )\n",
    "    else:\n",
    "        sampled_points_action = points_action\n",
    "        sampled_action_points = action_points\n",
    "\n",
    "    if \"sampled_ixs_anchor\" in model_outputs[0]:\n",
    "        ixs_anchor = model_outputs[0][\"sampled_ixs_anchor\"].unsqueeze(-1)\n",
    "        sampled_points_anchor = torch.take_along_dim(\n",
    "            points_anchor, ixs_anchor, dim=1\n",
    "        )\n",
    "        sampled_anchor_points = torch.take_along_dim(\n",
    "            anchor_points, ixs_anchor, dim=1\n",
    "        )\n",
    "    else:\n",
    "        sampled_points_anchor = points_anchor\n",
    "        sampled_anchor_points = anchor_points\n",
    "\n",
    "    pred_flow_action, pred_w_action = model.extract_flow_and_weight(x_action)\n",
    "    pred_flow_anchor, pred_w_anchor = model.extract_flow_and_weight(x_anchor)\n",
    "\n",
    "    pred_T_action = dualflow2pose(xyz_src=sampled_action_points, \n",
    "                                xyz_tgt=sampled_anchor_points,\n",
    "                                flow_src=pred_flow_action, \n",
    "                                flow_tgt=pred_flow_anchor,\n",
    "                                weights_src=pred_w_action, \n",
    "                                weights_tgt=pred_w_anchor,\n",
    "                                return_transform3d=True, \n",
    "                                normalization_scehme=\"softmax\",\n",
    "                                temperature=1)\n",
    "\n",
    "    pred_points_action = pred_T_action.transform_points(action_points)\n",
    "\n",
    "    # all_predicted_points.append(T1.inverse().transform_points(pred_points_action)[0].detach().cpu().numpy())\n",
    "    all_predicted_points.append(pred_points_action[0].detach().cpu().numpy())\n",
    "    \n",
    "# plot_multi_np(all_predicted_points)\n",
    "print(f'model_outputs[0].keys(): {model_outputs[0].keys()}')\n",
    "model_outputs[0]['pred_points_action'] = torch.Tensor(all_predicted_points[-1]).unsqueeze(0)\n",
    "print(f'pred_points_action.shape: {all_predicted_points[-1].shape}')\n",
    "plot_taxposed_embeddings(points_action=action_points[:1],\n",
    "                         points_anchor=anchor_points[:1],\n",
    "                         ans=model_outputs[0],\n",
    "                         hydra_cfg=cfg,)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_rpdiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
